{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "30\n",
      "34\n",
      "9\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# LSTM for shipment forecasting problem with regression framing\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM, GRU, LeakyReLU\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler, Normalizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras.optimizers as optimize\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from keras.regularizers import L1L2\n",
    "ADAM = optimize.Adam(lr=0.0002, beta_1=0.9, beta_2=.999, decay=0.00004)\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back):  #for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        #print(len(dataX))\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "        #print(len(dataY))\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv('../datasets/LSTM_PL29_Rev2.csv', usecols=[1], engine='python', skipfooter=0)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "#RobustScaler the dataset\n",
    "scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "dataset_df = DataFrame(dataset)\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * .77)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size+4,:], dataset[train_size+1:len(dataset)+1,:]\n",
    "print(len(dataset))\n",
    "print(train_size)\n",
    "print(len(train))\n",
    "print(test_size)\n",
    "print(len(test))\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(300, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "               kernel_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               recurrent_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               bias_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               activity_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               dropout=0.0, recurrent_dropout=0.0, \n",
    "               return_sequences=True, stateful=False, \n",
    "               use_bias=True, input_shape=(1, look_back)))\n",
    "\n",
    "model.add(LSTM(200, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "               kernel_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               recurrent_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               bias_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               activity_regularizer=L1L2(l1=0.00,l2=0.0), \n",
    "               dropout=0.0, recurrent_dropout=0.0, \n",
    "               return_sequences=True, stateful=False))\n",
    "\n",
    "model.add(LSTM(100, activation='tanh', recurrent_activation='sigmoid',\n",
    "               return_sequences=True, stateful=False, \n",
    "               kernel_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               recurrent_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               bias_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               activity_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               dropout=0.0))\n",
    "\n",
    "model.add(LSTM(50, activation='tanh', recurrent_activation='tanh',\n",
    "               return_sequences=False, stateful=False, \n",
    "               kernel_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               recurrent_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               bias_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               activity_regularizer=L1L2(l1=0.0,l2=0.0), \n",
    "               dropout=0.0))\n",
    "\n",
    "model.add(Dense(1, activation= 'linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=ADAM, metrics= ['mse'])\n",
    "\n",
    "history = model.fit(trainX, trainY, epochs=2500, batch_size=1, verbose=0, validation_data=(testX, testY), shuffle=False) \n",
    "\n",
    "#Saved Trained Model\n",
    "model.save(\"../datasets/LSTM_PL29_trained_model_Take4.h5\")\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "\n",
    "trainPredict_df = DataFrame(trainPredict)\n",
    "trainPredict_df.to_csv('../datasets/LSTM_trainPredict_dataset.csv')\n",
    "\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "\n",
    "testPredict_df = DataFrame(testPredict)\n",
    "testPredict_df.to_csv('../datasets/LSTM_testPredict_dataset.csv')\n",
    "\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(3):len(dataset)+1, :] = testPredict\n",
    "print(len(dataset))\n",
    "print(len(testPredict))\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.savefig('../datasets/forecast8.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
